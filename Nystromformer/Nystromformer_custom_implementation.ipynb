{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnca09_dMgtH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import time\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformations (grayscale, normalization, flattening)\n",
        "class CustomCIFAR10Dataset(Dataset):\n",
        "    def __init__(self, cifar_dataset, transform=None, flatten=False):\n",
        "        \"\"\"\n",
        "        Custom dataset wrapper for CIFAR-10.\n",
        "\n",
        "        Args:\n",
        "            cifar_dataset: The original CIFAR-10 dataset (loaded with torchvision.datasets).\n",
        "            transform: Transformations to apply to the images.\n",
        "            flatten: Whether to flatten the images to 1D.\n",
        "        \"\"\"\n",
        "        self.cifar_dataset = cifar_dataset\n",
        "        self.transform = transform\n",
        "        self.flatten = flatten\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.cifar_dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.cifar_dataset[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.flatten:\n",
        "            image = image.view(-1)  # Flatten [1, 32, 32] to [1024]\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Define the preprocessing transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
        "    transforms.ToTensor(),                       # Convert to tensor\n",
        "    transforms.Normalize((0.5,), (0.5,))         # Normalize to [-1, 1]\n",
        "])\n"
      ],
      "metadata": {
        "id": "avRU_sMIMnfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_cifar_train = CIFAR10(root='./data', train=True, download=True)\n",
        "original_cifar_test = CIFAR10(root='./data', train=False, download=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiP8op0CNrZR",
        "outputId": "fc005bbc-304a-4167-cc71-22c56a835bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 49.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomCIFAR10Dataset(original_cifar_train, transform=transform, flatten=True)\n",
        "test_dataset = CustomCIFAR10Dataset(original_cifar_test, transform=transform, flatten=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "WEaLXZXrNM0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def segment_means(input_seq, m):\n",
        "    \"\"\"Compute landmarks as segment means.\"\"\"\n",
        "    batch_size, seq_len = input_seq.shape  # Correctly interpret dimensions\n",
        "    segment_size = seq_len // m\n",
        "    assert segment_size > 0, f\"Segment size must be greater than 0. seq_len={seq_len}, m={m}\"\n",
        "\n",
        "    # Compute segment means for each sequence in the batch\n",
        "    landmarks = torch.stack([\n",
        "        torch.stack([\n",
        "            input_seq[b, i * segment_size:(i + 1) * segment_size].mean(dim=0)\n",
        "            for i in range(m)\n",
        "        ]) for b in range(batch_size)\n",
        "    ])\n",
        "    return landmarks\n",
        "\n",
        "def nystrom_attention(Q, K, V, num_landmarks):\n",
        "    \"\"\"Approximate attention using the Nyström method.\"\"\"\n",
        "    batch_size, seq_len = Q.shape\n",
        "\n",
        "    # Normalize Q and K for numerical stability\n",
        "    Q = Q / (Q.norm(dim=-1, keepdim=True) + 1e-6)\n",
        "    K = K / (K.norm(dim=-1, keepdim=True) + 1e-6)\n",
        "\n",
        "    # Step 1: Compute landmarks\n",
        "    K_landmarks = segment_means(K, num_landmarks)  # Shape: [batch_size, num_landmarks]\n",
        "    Q_landmarks = segment_means(Q, num_landmarks)  # Shape: [batch_size, num_landmarks]\n",
        "\n",
        "    # Add a third dimension to allow batch matrix multiplication\n",
        "    K_landmarks = K_landmarks.unsqueeze(-1)  # Shape: [batch_size, num_landmarks, 1]\n",
        "    Q_landmarks = Q_landmarks.unsqueeze(-1)  # Shape: [batch_size, num_landmarks, 1]\n",
        "\n",
        "    # Step 2: Compute scaled attention components\n",
        "    scale = Q_landmarks.size(-2) ** 0.5  # Scale factor for stability\n",
        "    A = torch.softmax((Q_landmarks @ K_landmarks.transpose(-2, -1)) / scale, dim=-1)  # Shape: [batch_size, num_landmarks, num_landmarks]\n",
        "    F = torch.softmax((Q.unsqueeze(-1) @ K_landmarks.transpose(-2, -1)) / scale, dim=-1)  # Shape: [batch_size, seq_len, num_landmarks]\n",
        "    B = torch.softmax((Q_landmarks @ K.unsqueeze(-1).transpose(-2, -1)) / scale, dim=-1)  # Shape: [batch_size, num_landmarks, seq_len]\n",
        "\n",
        "    # Step 3: Regularize and compute pseudoinverse\n",
        "    epsilon = 1e-6\n",
        "    A = A + epsilon * torch.eye(A.size(-1), device=A.device).unsqueeze(0)  # Avoid in-place operation\n",
        "    A_pinv = torch.linalg.pinv(A)  # Shape: [batch_size, num_landmarks, num_landmarks]\n",
        "\n",
        "    # Step 4: Combine matrices\n",
        "    S_hat = torch.bmm(F, torch.bmm(A_pinv, B))  # Shape: [batch_size, seq_len, seq_len]\n",
        "    output = torch.bmm(S_hat, V.unsqueeze(-1)).squeeze(-1)  # Shape: [batch_size, seq_len]\n",
        "\n",
        "    return output\n",
        "\n",
        "class NystromformerLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_landmarks):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_landmarks = num_landmarks\n",
        "\n",
        "        self.query = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value = nn.Linear(embed_dim, embed_dim)\n",
        "        self.out = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Project to Q, K, V\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        # Apply Nyström attention\n",
        "        output = nystrom_attention(Q, K, V, self.num_landmarks)\n",
        "\n",
        "        # Final projection\n",
        "        return self.out(output)\n",
        "\n",
        "# Full Model with Classification Head\n",
        "class NystromformerModel(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_landmarks, num_classes):\n",
        "        super().__init__()\n",
        "        self.nystromformer = NystromformerLayer(embed_dim, num_heads, num_landmarks)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.nystromformer(x)\n",
        "        return self.classifier(x)"
      ],
      "metadata": {
        "id": "O7LUh4CHNuaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = NystromformerModel(embed_dim=1024, num_heads=4, num_landmarks=64, num_classes=10)"
      ],
      "metadata": {
        "id": "V6Ml_21TO34O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHSsUQPe8Sjd",
        "outputId": "4f8c8949-93fa-49e0-8844-a16327f3f57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NystromformerModel(\n",
              "  (nystromformer): NystromformerLayer(\n",
              "    (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (out): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=1024, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, criterion, optimizer, device, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()  # Track epoch start time\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        print(f\"\\nEpoch [{epoch + 1}/{epochs}]\")\n",
        "        with tqdm(total=len(train_loader), desc=\"Training Progress\") as pbar:  # Progress bar\n",
        "            for images, labels in train_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Backward pass\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                # Gradient clipping\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                # Optimizer step\n",
        "                optimizer.step()\n",
        "\n",
        "                # Metrics\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({\"loss\": loss.item(), \"accuracy\": 100. * correct / total})\n",
        "                pbar.update(1)\n",
        "\n",
        "        # Epoch timing\n",
        "        end_time = time.time()\n",
        "        epoch_duration = end_time - start_time\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}] completed in {epoch_duration:.2f} seconds.\")\n",
        "        print(f\"  Loss: {total_loss:.4f}, Accuracy: {100. * correct / total:.2f}%\")"
      ],
      "metadata": {
        "id": "k94cPOZNU79N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f\"Test Loss: {total_loss:.4f}, Test Accuracy: {correct / total:.4f}\")\n"
      ],
      "metadata": {
        "id": "-GGLPvCp8m75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, device, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9SA2K6I8pYx",
        "outputId": "b1f4334b-4279-4d68-f193-59cf8c917657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch [1/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:16<00:00,  1.83it/s, loss=2.07, accuracy=15.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10] completed in 856.39 seconds.\n",
            "  Loss: 3482.2556, Accuracy: 15.43%\n",
            "\n",
            "Epoch [2/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:15<00:00,  1.83it/s, loss=2.33, accuracy=15.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10] completed in 855.68 seconds.\n",
            "  Loss: 3475.1181, Accuracy: 15.33%\n",
            "\n",
            "Epoch [3/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:16<00:00,  1.82it/s, loss=2.19, accuracy=15.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10] completed in 856.70 seconds.\n",
            "  Loss: 3479.2975, Accuracy: 15.38%\n",
            "\n",
            "Epoch [4/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:20<00:00,  1.82it/s, loss=2.48, accuracy=15.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10] completed in 860.07 seconds.\n",
            "  Loss: 3492.9830, Accuracy: 15.33%\n",
            "\n",
            "Epoch [5/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:23<00:00,  1.81it/s, loss=2.29, accuracy=15.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10] completed in 863.59 seconds.\n",
            "  Loss: 3482.4101, Accuracy: 15.50%\n",
            "\n",
            "Epoch [6/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:08<00:00,  1.84it/s, loss=2.1, accuracy=15.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10] completed in 848.98 seconds.\n",
            "  Loss: 3482.8727, Accuracy: 15.55%\n",
            "\n",
            "Epoch [7/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:08<00:00,  1.84it/s, loss=2.2, accuracy=15.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10] completed in 848.84 seconds.\n",
            "  Loss: 3481.3598, Accuracy: 15.26%\n",
            "\n",
            "Epoch [8/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:13<00:00,  1.83it/s, loss=2.31, accuracy=15.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10] completed in 853.09 seconds.\n",
            "  Loss: 3487.9342, Accuracy: 15.28%\n",
            "\n",
            "Epoch [9/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:16<00:00,  1.83it/s, loss=2.26, accuracy=15.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10] completed in 856.16 seconds.\n",
            "  Loss: 3474.2520, Accuracy: 15.62%\n",
            "\n",
            "Epoch [10/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress: 100%|██████████| 1563/1563 [14:08<00:00,  1.84it/s, loss=2.22, accuracy=15.3]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10] completed in 848.76 seconds.\n",
            "  Loss: 3482.2127, Accuracy: 15.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader, criterion, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrQ17ajC813t",
        "outputId": "c7fadf4d-34cf-4385-e11e-a06021473734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 687.5670, Test Accuracy: 0.1505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HipxTlvtVDsu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}